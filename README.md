Emotion Detection System using Face, Voice, Sign Language, and Diary
An integrated multi-modal Emotion Detection System built using Python that leverages facial expressions, voice tone, sign language, and written text (diary) to understand and classify human emotions. This project explores human-computer interaction, assistive technology, and AI-powered emotional intelligence.

Features
Facial Emotion Detection 
Uses computer vision (OpenCV, Deep Learning) to classify emotions like happy, sad, angry, surprised, etc., from facial expressions.
Voice Emotion Recognition
Detects emotion from voice tone using feature extraction techniques (MFCC, chroma) and machine learning models.
Sign Language Emotion Interpretation
Translates basic emotional signs using hand gesture recognition, enabling emotion detection for hearing or speech-impaired users.
Diary-Based Sentiment Analysis 
Analyzes emotional tone from written text entries (diary/journals) using NLP and sentiment classification models.

Tech Stack
| Component         | Tools/Libraries Used                          |
|------------------|------------------------------------------------|
| Language          | Python 3.x                                    |
| Facial Detection  | OpenCV, Dlib, TensorFlow/Keras, Haarcascade   |
| Voice Detection   | LibROSA, SoundFile, NumPy, scikit-learn       |
| Sign Language     | MediaPipe, OpenCV, Custom CNN                 |
| Diary/NLP         | NLTK / spaCy / TextBlob / Transformers        |
| GUI (Optional)    | Tkinter / Streamlit / Flask                   |
| Visualization     | Matplotlib, Seaborn                           |
